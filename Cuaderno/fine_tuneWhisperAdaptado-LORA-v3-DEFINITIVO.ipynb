{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ade6b4e",
   "metadata": {},
   "source": [
    "# Instalación de las dependencias necesarias para el proyecto"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3aed67ba",
   "metadata": {},
   "source": [
    " # Instalamos las librerías principales:\n",
    "# - transformers: para trabajar con modelos preentrenados de Hugging Face.\n",
    "# - datasets: para acceder a conjuntos de datos preconfigurados.\n",
    "# - librosa: para procesamiento de audio.\n",
    "# - evaluate: para métricas de evaluación.\n",
    "# - jiwer: para evaluar transcripciones de texto (WER - Word Error Rate).\n",
    "# - gradio: para crear interfaces gráficas para pruebas interactivas.\n",
    "# - bitsandbytes y accelerate: optimización y soporte para hardware de alto rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes==0.37 accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645bf65-0538-4fb2-a75b-68594f87b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers==4.47.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a81e8-6338-4ef7-aafb-0f57c22ef666",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate==1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518a2a2-18c9-4274-b59d-04c9abda8b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets==2.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13988ed-ab0c-4de0-88be-b0e63043aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jiwer==3.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005051bf-68c8-4ed8-8bfe-6627e4bb1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install librosa==0.10.2.post1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405c748-352d-48fe-baca-c01bf5b43cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install evaluate==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa42be-f5f0-400b-988a-80f168ba4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bitsandbytes==0.45.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923b9bb-7832-4f33-9d66-73596111b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8e85f-f6ed-4892-8bd1-14f82c7597de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c447f2-d3a8-43a8-8f3b-13add12c8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ba760cd",
   "metadata": {},
   "source": [
    "# Instalación adicional de PEFT desde el repositorio de Hugging Face:\n",
    "# Esto asegura que tengamos la última versión en caso de conflictos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553366e-3793-4279-81d8-1c6cd6cb2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7269f-ffc9-4097-bb9d-e8cc1639263d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install peft==0.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b71f86-5bc9-44ec-8ae5-e7c43e7aaecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.47.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft\n",
      "---\n",
      "Name: datasets\n",
      "Version: 2.20.0\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages\n",
      "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyarrow-hotfix, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: evaluate\n",
      "---\n",
      "Name: librosa\n",
      "Version: 0.10.2.post1\n",
      "Summary: Python module for audio and music processing\n",
      "Home-page: https://librosa.org\n",
      "Author: Brian McFee, librosa development team\n",
      "Author-email: brian.mcfee@nyu.edu\n",
      "License: ISC\n",
      "Location: /home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages\n",
      "Requires: audioread, decorator, joblib, lazy-loader, msgpack, numba, numpy, pooch, scikit-learn, scipy, soundfile, soxr, typing-extensions\n",
      "Required-by: \n",
      "---\n",
      "Name: evaluate\n",
      "Version: 0.4.3\n",
      "Summary: HuggingFace community-driven open-source library of evaluation\n",
      "Home-page: https://github.com/huggingface/evaluate\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: leandro@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages\n",
      "Requires: datasets, dill, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, requests, tqdm, xxhash\n",
      "Required-by: \n",
      "---\n",
      "Name: jiwer\n",
      "Version: 3.0.5\n",
      "Summary: Evaluate your speech-to-text system with similarity measures such as word error rate (WER)\n",
      "Home-page: https://github.com/jitsi/jiwer\n",
      "Author: Nik Vaessen\n",
      "Author-email: nikvaes@gmail.com\n",
      "License: Apache-2.0\n",
      "Location: /home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages\n",
      "Requires: click, rapidfuzz\n",
      "Required-by: \n",
      "---\n",
      "Name: gradio\n",
      "Version: 5.14.0\n",
      "Summary: Python library for easily interacting with trained machine learning models\n",
      "Home-page: https://github.com/gradio-app/gradio\n",
      "Author: \n",
      "Author-email: Abubakar Abid <gradio-team@huggingface.co>, Ali Abid <gradio-team@huggingface.co>, Ali Abdalla <gradio-team@huggingface.co>, Dawood Khan <gradio-team@huggingface.co>, Ahsen Khaliq <gradio-team@huggingface.co>, Pete Allen <gradio-team@huggingface.co>, Ömer Faruk Özdemir <gradio-team@huggingface.co>, Freddy A Boulton <gradio-team@huggingface.co>, Hannah Blair <gradio-team@huggingface.co>\n",
      "License: \n",
      "Location: /home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages\n",
      "Requires: aiofiles, anyio, fastapi, ffmpy, gradio-client, httpx, huggingface-hub, jinja2, markupsafe, numpy, orjson, packaging, pandas, pillow, pydantic, pydub, python-multipart, pyyaml, ruff, safehttpx, semantic-version, starlette, tomlkit, typer, typing-extensions, uvicorn\n",
      "Required-by: \n",
      "---\n",
      "Name: bitsandbytes\n",
      "Version: 0.45.0\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\n",
      "Home-page: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
      "Author: Tim Dettmers\n",
      "Author-email: dettmers@cs.washington.edu\n",
      "License: MIT\n",
      "Location: /home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages\n",
      "Requires: numpy, torch, typing_extensions\n",
      "Required-by: \n",
      "---\n",
      "Name: accelerate\n",
      "Version: 1.2.1\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: peft\n",
      "---\n",
      "Name: peft\n",
      "Version: 0.14.0\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: benjamin@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages\n",
      "Requires: accelerate, huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers datasets librosa evaluate jiwer gradio bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8f8c7-ba37-4f25-9107-0feb85229a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f88e71-4c79-4c03-bacd-df448ca807b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Iniciar sesión con tu token\n",
    "#login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc27c863-14a1-45b8-99c9-6dfe69fa0466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manucerrejon/.local/lib/python3.10/site-packages/triton/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "print(triton.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cc4a1",
   "metadata": {},
   "source": [
    "### Verificación de la Versión de `transformers`\n",
    "\n",
    "En esta celda se importa la librería **transformers** y se imprime su versión instalada. Esto es importante porque ciertas funcionalidades pueden variar entre versiones. \n",
    "\n",
    "Si no se obtiene la versión esperada, puede ser necesario reinstalar o actualizar la librería utilizando:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91dbffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47.1\n"
     ]
    }
   ],
   "source": [
    "# Importamos la librería transformers para trabajar con modelos preentrenados de Hugging Face.\n",
    "# A continuación, imprimimos la versión instalada para verificar que es la adecuada.\n",
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c7aa2",
   "metadata": {},
   "source": [
    "### Verificación de la Versión de `peft`\n",
    "\n",
    "En esta celda se verifica que la versión instalada de la librería **PEFT** sea la 0.8. PEFT es una herramienta fundamental para realizar ajustes eficientes en modelos de machine learning grandes, especialmente en hardware limitado.\n",
    "\n",
    "**Importante:** Si la versión mostrada no es la 0.8, podría haber problemas de compatibilidad. Para asegurarse de tener la versión correcta, utilice el siguiente comando para instalarla directamente desde el repositorio de Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8fcf99b-d5d2-4b3d-91da-f83d797e251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14.0\n"
     ]
    }
   ],
   "source": [
    "# Importamos la librería PEFT (Parameter-Efficient Fine-Tuning) para ajustar modelos de manera eficiente.\n",
    "# Comprobamos que la versión instalada sea la 0.8, ya que algunas características clave pueden depender de esta versión específica.\n",
    "import peft\n",
    "\n",
    "print(peft.__version__)  # Debe mostrar '0.8'\n",
    "\n",
    "#help(peft.prepare_model_for_kbit_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76076f4b",
   "metadata": {},
   "source": [
    "### Verificación de la Conexión a GPU\n",
    "\n",
    "Esta celda verifica si hay una GPU disponible para el entorno de ejecución utilizando el comando `nvidia-smi`, que proporciona información detallada sobre las GPUs NVIDIA conectadas.\n",
    "\n",
    "- Si no se detecta ninguna GPU, el mensaje `\"No estás conectado a una GPU\"` se mostrará.\n",
    "- Si hay una GPU disponible, se imprimirá su información detallada, incluyendo modelo, memoria utilizada y otros detalles técnicos.\n",
    "\n",
    "Esto es particularmente útil para asegurarse de que el entorno tiene los recursos de hardware necesarios para ejecutar tareas que requieren aceleración por GPU, como entrenamiento de modelos grandes o procesamiento intensivo de datos.\n",
    "\n",
    "**Nota:** Si no se detecta una GPU y el proyecto la requiere, asegúrate de habilitarla en la configuración del entorno (por ejemplo, en Google Colab, ve a `Entorno de ejecución > Cambiar tipo de entorno de ejecución > Acelerador de hardware > GPU`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08923281-bab5-4707-b63d-5b3f4450dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Ahora puedes mover tus tensores a la GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb7dab48-9432-4e68-ada5-c7620b71cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 13 11:40:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   44C    P8              8W /  450W |    2404MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3050        Off |   00000000:05:00.0 Off |                  N/A |\n",
      "|  0%   42C    P8             12W /  130W |      17MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1975      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A     21033      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A     22795      C   ...cudaenv_manucerrejon2025/bin/python       2378MiB |\n",
      "|    1   N/A  N/A      1975      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A     21033      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Verificamos la información de la GPU conectada, si está disponible.\n",
    "# Utilizamos el comando `nvidia-smi` para obtener detalles sobre la GPU.\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "\n",
    "# Si no se detecta una GPU, se informa al usuario.\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('No estás conectado a una GPU')\n",
    "else:\n",
    "    # Mostramos los detalles de la GPU si está disponible.\n",
    "    print(gpu_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6983bc12-b826-4daf-ab2e-51c6e38657af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5ab6a",
   "metadata": {},
   "source": [
    "### Importación de Librerías y Configuración Inicial\n",
    "\n",
    "Esta celda prepara el entorno para el procesamiento de datos y la configuración del modelo de transcripción. A continuación se detallan las acciones realizadas:\n",
    "\n",
    "1. **Importación de librerías:**\n",
    "   - **os, json:** Manejo de archivos y rutas.\n",
    "   - **torch:** Herramientas para el uso de PyTorch.\n",
    "   - **datasets:** Manejo de conjuntos de datos, en este caso, con soporte para audio.\n",
    "   - **transformers:** Herramientas relacionadas con el modelo Whisper de Hugging Face.\n",
    "   - **soundfile, pandas:** Utilidades para el procesamiento de audio y datos tabulares.\n",
    "\n",
    "2. **Configuración del idioma y tarea:**\n",
    "   - El idioma seleccionado es **español** (`Spanish`).\n",
    "   - La tarea es **transcripción** (`transcribe`), pero podría cambiarse a otras como traducción si el modelo lo soporta.\n",
    "\n",
    "3. **Selección del modelo Whisper:**\n",
    "   - Por defecto, se utiliza el modelo `openai/whisper-large-v3`. \n",
    "   - Otros modelos (`medium`, `small`) también están disponibles, dependiendo de los recursos y necesidades.\n",
    "\n",
    "4. **Rutas de los archivos:**\n",
    "   - Se definen rutas relativas para los datos de audio y las transcripciones generadas.\n",
    "   - **Archivo JSON:** Contiene la configuración o los metadatos necesarios para la tarea.\n",
    "\n",
    "5. **Validación del archivo JSON:**\n",
    "   - Se verifica que el archivo JSON exista antes de proceder, levantando una excepción si no está presente. Esto asegura que el flujo no se rompa inesperadamente.\n",
    "\n",
    "**Nota:** Este paso asegura que el entorno esté correctamente configurado para procesar datos y entrenar el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e8423ec-250f-4318-81c0-8eee14a42e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias para el procesamiento y entrenamiento\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    WhisperTokenizer,\n",
    "    TrainingArguments,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorWithPadding,\n",
    "    WhisperProcessor,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_metric, DatasetDict\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import evaluate \n",
    "import librosa\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# Configuración del idioma y la tarea a realizar\n",
    "language = \"English\"  # Idioma principal para transcripciones\n",
    "task = \"transcribe\"   # Tarea específica para el modelo Whisper (puede ser transcripción, traducción, etc.)\n",
    "\n",
    "# Selección del modelo Whisper (se puede ajustar según la necesidad)\n",
    "# modelo = \"openai/whisper-medium\"\n",
    "modelo = \"openai/whisper-large\"\n",
    "# modelo = \"openai/whisper-small\"\n",
    "# modelo = \"./whisper-finetuned-large-Puerto\"\n",
    "\n",
    "# Configuración de rutas de los directorios y archivos\n",
    "# Rutas para entrenamiento\n",
    "audio_dir = \"./train\"                       # Directorio con los archivos de audio\n",
    "transcriptions_dir = \"./train/transcripciones\"  # Directorio donde se almacenarán las transcripciones\n",
    "json_path_train = \"./train_data.json\"    # Ruta al archivo JSON con datos de entrenamiento\n",
    "\n",
    "# Rutas para test\n",
    "audio_dir_test = \"./test/\"                          # Directorio con los archivos de audio para pruebas\n",
    "transcriptions_dir_test = \"./test/transcripciones/\" # Directorio para transcripciones de prueba\n",
    "json_path_test = \"./test_data.json\"                      # Ruta al archivo JSON con datos de prueba\n",
    "\n",
    "# Verificar si los archivos JSON existen\n",
    "if not os.path.exists(json_path_train):\n",
    "    raise FileNotFoundError(f\"El archivo JSON no se encontró en la ruta: {json_path_train}\")\n",
    "if not os.path.exists(json_path_test):\n",
    "    raise FileNotFoundError(f\"El archivo JSON no se encontró en la ruta: {json_path_test}\")\n",
    "\n",
    "# Cargar la métrica (ejemplo: Word Error Rate - WER)\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87460e9",
   "metadata": {},
   "source": [
    "### Carga de Datos desde un Archivo JSON\n",
    "\n",
    "Esta celda define y utiliza una función para cargar datos de audio y transcripciones desde un archivo JSON. A continuación se explican los pasos realizados:\n",
    "\n",
    "1. **Importación de librerías:** \n",
    "   - **librosa** se utiliza más adelante en el proyecto para procesar audio.\n",
    "   - **os** y **json** son esenciales para manejar rutas y leer el archivo JSON.\n",
    "\n",
    "2. **Definición de la función `load_data_from_json`:**\n",
    "   - Lee un archivo JSON que contiene información sobre los nombres de los archivos de audio y sus transcripciones.\n",
    "   - Verifica que los archivos de audio y transcripciones existen en los directorios especificados.\n",
    "   - Devuelve un diccionario con dos listas:\n",
    "     - `audio`: Rutas de los archivos de audio.\n",
    "     - `transcription`: Contenido de las transcripciones correspondientes.\n",
    "\n",
    "3. **Control de errores:**\n",
    "   - Si algún archivo falta, se muestra una lista de los archivos faltantes en la salida.\n",
    "\n",
    "4. **Cargar los datos de entrenamiento:**\n",
    "   - Se usa la función para cargar los datos desde el archivo JSON definido previamente.\n",
    "   - Se imprime el número de archivos de audio y transcripciones cargados correctamente.\n",
    "\n",
    "5. **Mostrar una muestra de los datos:**\n",
    "   - Para validar que los datos se cargaron correctamente, se imprimen las primeras tres entradas de audio y transcripciones.\n",
    "\n",
    "Esta etapa asegura que los datos están preparados para su uso en el entrenamiento del modelo **Whisper**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02da87ef-c2e8-4871-9324-d28abb53b08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de archivos de audio cargados en data_train: 531\n",
      "Número de transcripciones cargadas en data_train: 531\n",
      "\n",
      "Muestra de los datos cargados para entrenamiento:\n",
      "Audio 1: ./train/17329-20230629-0004-002.wav\n",
      "Transcription 1: Can we get a split, please?\n",
      "\n",
      "Audio 2: ./train/17329-20230629-0004-004.wav\n",
      "Transcription 2: Copy, 5-6.\n",
      "\n",
      "Audio 3: ./train/17329-20230629-0004-005.wav\n",
      "Transcription 3: 6-S, port side.\n",
      "\n",
      "Número de archivos de audio cargados en data_test: 131\n",
      "Número de transcripciones cargadas en data_test: 131\n",
      "\n",
      "Muestra de los datos cargados para test:\n",
      "Audio 1: ./test/17329-20230629-0004-001.wav\n",
      "Transcription 1: Yeah, we're all finished with drills.\n",
      "\n",
      "Audio 2: ./test/17329-20230629-0004-003.wav\n",
      "Transcription 2: Sure, 5-6.\n",
      "\n",
      "Audio 3: ./test/17329-20230629-0004-011.wav\n",
      "Transcription 3: I've got to give that my hand.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import soundfile as sf\n",
    "\n",
    "# Función para cargar los datos desde el archivo JSON\n",
    "def load_data_from_json(json_path, audio_dir, transcriptions_dir):\n",
    "    \"\"\"\n",
    "    Carga los datos de audio y transcripciones desde un archivo JSON.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Ruta al archivo JSON que contiene los nombres de los archivos.\n",
    "        audio_dir (str): Directorio que contiene los archivos de audio.\n",
    "        transcriptions_dir (str): Directorio que contiene las transcripciones.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con dos claves:\n",
    "              - \"audio\": Lista de rutas a los archivos de audio.\n",
    "              - \"transcription\": Lista de transcripciones correspondientes.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data_json = json.load(f)\n",
    "    \n",
    "    data = {\"audio\": [], \"transcription\": []}\n",
    "    missing_files = []  # Lista para rastrear archivos faltantes\n",
    "    invalid_transcriptions = []  # Lista para rastrear transcripciones vacías o inválidas\n",
    "\n",
    "    for entry in data_json:\n",
    "        # Obtener nombres de los archivos desde el JSON\n",
    "        audio_file = entry[\"nombreFichero\"]\n",
    "        transcription_file = entry[\"transcripcionCorrecta\"]\n",
    "        \n",
    "        # Construir rutas completas para los archivos de audio y transcripciones\n",
    "        audio_path = os.path.join(audio_dir, audio_file)\n",
    "        transcription_path = os.path.join(transcriptions_dir, transcription_file)\n",
    "        \n",
    "        # Verificar si ambos archivos existen\n",
    "        if os.path.exists(audio_path) and os.path.exists(transcription_path):\n",
    "            # Leer y limpiar la transcripción\n",
    "            with open(transcription_path, 'r') as f:\n",
    "                transcription = f.read().strip()\n",
    "            \n",
    "            # Verificar que la transcripción no esté vacía o contenga solo espacios\n",
    "            if transcription:\n",
    "                # Comprobar si el archivo de audio tiene un formato válido\n",
    "                try:\n",
    "                    # Intenta cargar el archivo de audio\n",
    "                    _ = sf.read(audio_path)\n",
    "                    data[\"audio\"].append(audio_path)\n",
    "                    data[\"transcription\"].append(transcription)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al cargar el archivo de audio {audio_path}: {e}\")\n",
    "                    missing_files.append(audio_path)\n",
    "            else:\n",
    "                invalid_transcriptions.append(transcription_path)\n",
    "        else:\n",
    "            # Agregar archivos faltantes a la lista\n",
    "            if not os.path.exists(audio_path):\n",
    "                missing_files.append(audio_path)\n",
    "            if not os.path.exists(transcription_path):\n",
    "                missing_files.append(transcription_path)\n",
    "    \n",
    "    # Mostrar los archivos faltantes y las transcripciones inválidas\n",
    "    if missing_files:\n",
    "        print(\"Archivos faltantes:\")\n",
    "        for missing in missing_files:\n",
    "            print(missing)\n",
    "    if invalid_transcriptions:\n",
    "        print(\"Transcripciones inválidas (vacías):\")\n",
    "        for invalid in invalid_transcriptions:\n",
    "            print(invalid)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Cargar los datos de entrenamiento\n",
    "data_train = load_data_from_json(json_path_train, audio_dir, transcriptions_dir)\n",
    "\n",
    "# Verificar cuántos datos se han cargado para entrenamiento\n",
    "print(f\"Número de archivos de audio cargados en data_train: {len(data_train['audio'])}\")\n",
    "print(f\"Número de transcripciones cargadas en data_train: {len(data_train['transcription'])}\")\n",
    "\n",
    "# Verificar que los audios y las transcripciones de entrenamiento están correctamente cargados\n",
    "# Imprimir una muestra de los datos de entrenamiento cargados\n",
    "print(\"\\nMuestra de los datos cargados para entrenamiento:\")\n",
    "for i in range(min(3, len(data_train['audio']))):  # Mostrar las primeras 3 muestras\n",
    "    audio = data_train[\"audio\"][i]\n",
    "    transcription = data_train[\"transcription\"][i]\n",
    "    print(f\"Audio {i+1}: {audio}\")\n",
    "    print(f\"Transcription {i+1}: {transcription}\\n\")\n",
    "\n",
    "# Cargar los datos de prueba\n",
    "data_test = load_data_from_json(json_path_test, audio_dir_test, transcriptions_dir_test)\n",
    "\n",
    "# Verificar cuántos datos se han cargado para test\n",
    "print(f\"Número de archivos de audio cargados en data_test: {len(data_test['audio'])}\")\n",
    "print(f\"Número de transcripciones cargadas en data_test: {len(data_test['transcription'])}\")\n",
    "\n",
    "# Verificar que los audios y las transcripciones de test están correctamente cargados\n",
    "# Imprimir una muestra de los datos de prueba cargados\n",
    "print(\"\\nMuestra de los datos cargados para test:\")\n",
    "for i in range(min(3, len(data_test['audio']))):  # Mostrar las primeras 3 muestras\n",
    "    audio = data_test[\"audio\"][i]\n",
    "    transcription = data_test[\"transcription\"][i]\n",
    "    print(f\"Audio {i+1}: {audio}\")\n",
    "    print(f\"Transcription {i+1}: {transcription}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa283c",
   "metadata": {},
   "source": [
    "### División de los Datos en Entrenamiento y Prueba\n",
    "\n",
    "En esta celda, se realiza una división simple de los datos para separar un subconjunto pequeño de prueba de los datos de entrenamiento. Esto es útil para evaluar el rendimiento del modelo en un conjunto de datos que no ha sido utilizado durante el entrenamiento.\n",
    "\n",
    "#### Pasos realizados:\n",
    "1. **Creación del conjunto de prueba (`data_test`):**\n",
    "   - Se seleccionan las primeras 20 muestras de los datos de audio y transcripciones de `data_train`.\n",
    "   - Este conjunto será utilizado para pruebas o validación posterior.\n",
    "\n",
    "2. **Actualización del conjunto de entrenamiento (`data_train`):**\n",
    "   - Se eliminan las primeras 20 muestras de `data_train`, dejando el resto de los datos para el entrenamiento del modelo.\n",
    "\n",
    "3. **Verificación de los tamaños:**\n",
    "   - Se imprimen los tamaños de ambos conjuntos para asegurarse de que la división se realizó correctamente.\n",
    "\n",
    "#### Nota:\n",
    "- **Por qué 20 muestras:** Este número es arbitrario y puede ajustarse según la cantidad de datos disponibles o los requerimientos del experimento.\n",
    "- Es importante que el conjunto de prueba sea representativo del conjunto completo para obtener resultados fiables durante la evaluación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "750a5064-65ac-4044-b666-be9b9f1ca567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dividimos los datos en un conjunto de prueba (primeros 20 datos) y el resto para entrenamiento.\n",
    "# # Creamos el conjunto de prueba con las primeras 20 muestras\n",
    "# data_test = {\n",
    "#     \"audio\": data_train[\"audio\"][:20],\n",
    "#     \"transcription\": data_train[\"transcription\"][:20]\n",
    "# }\n",
    "\n",
    "# # Modificamos el conjunto de entrenamiento eliminando las primeras 20 muestras\n",
    "# data_train = {\n",
    "#     \"audio\": data_train[\"audio\"][20:],  # Usamos 20 en lugar de 21 para evitar perder una muestra\n",
    "#     \"transcription\": data_train[\"transcription\"][20:]\n",
    "# }\n",
    "\n",
    "# # Verificar el tamaño de ambos conjuntos\n",
    "# print(f\"Número de datos en el conjunto de prueba (data_test): {len(data_test['audio'])}\")\n",
    "# print(f\"Número de datos en el conjunto de entrenamiento (data_train): {len(data_train['audio'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727763b",
   "metadata": {},
   "source": [
    "### Conversión de los Datos en Formatos Compatibles con Hugging Face\n",
    "\n",
    "En esta celda, se transforma la estructura de los datos en un formato compatible con la biblioteca **Hugging Face Datasets**, lo que facilita su uso para tareas de procesamiento y entrenamiento de modelos. A continuación, se explican los pasos realizados:\n",
    "\n",
    "1. **Creación de DataFrames con pandas:**\n",
    "   - Se utiliza **pandas** para crear tablas con columnas `audio` (rutas de los archivos de audio) y `transcription` (las transcripciones correspondientes).\n",
    "   - Esto permite una organización clara de los datos.\n",
    "\n",
    "2. **Conversión a `Dataset` de Hugging Face:**\n",
    "   - Los DataFrames se convierten en objetos `Dataset`, que son la estructura principal utilizada por la biblioteca Hugging Face para manejar datos.\n",
    "\n",
    "3. **Creación de un `DatasetDict`:**\n",
    "   - Se agrupan los conjuntos de entrenamiento (`train`) y prueba (`test`) en un solo objeto `DatasetDict` para facilitar su manejo.\n",
    "\n",
    "4. **Conversión de la columna `audio`:**\n",
    "   - La columna `audio` se transforma utilizando `Audio` de Hugging Face, especificando una frecuencia de muestreo de **16 kHz**.\n",
    "   - Esto asegura que los archivos de audio sean cargados y procesados de manera uniforme.\n",
    "\n",
    "5. **Verificación del conjunto de datos:**\n",
    "   - Se imprime la estructura del conjunto de datos de entrenamiento (`data[\"train\"]`) para comprobar que la conversión fue exitosa.\n",
    "\n",
    "#### Ventaja de esta transformación:\n",
    "Este formato es ideal para integrarse con modelos como **Whisper**, ya que Hugging Face proporciona herramientas para manejar datasets de audio con columnas sincronizadas para entrenamiento y evaluación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d97aa688-8409-4386-b262-b52c1c89099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'transcription'],\n",
      "    num_rows: 531\n",
      "})\n",
      "Dataset({\n",
      "    features: ['audio', 'transcription'],\n",
      "    num_rows: 131\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convertir los datos de entrenamiento y prueba a DataFrames de pandas\n",
    "df_train = pd.DataFrame({\n",
    "    \"audio\": data_train[\"audio\"],          # Rutas de los archivos de audio para entrenamiento\n",
    "    \"transcription\": data_train[\"transcription\"]  # Transcripciones correspondientes\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    \"audio\": data_test[\"audio\"],           # Rutas de los archivos de audio para prueba\n",
    "    \"transcription\": data_test[\"transcription\"]   # Transcripciones correspondientes\n",
    "})\n",
    "\n",
    "# Convertir los DataFrames de pandas a objetos Dataset de Hugging Face\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Crear un DatasetDict para organizar los conjuntos de entrenamiento y prueba\n",
    "data = DatasetDict({\n",
    "    \"train\": train_dataset,  # Conjunto de entrenamiento\n",
    "    \"test\": test_dataset     # Conjunto de prueba\n",
    "})\n",
    "\n",
    "# Convertir la columna \"audio\" a un formato compatible con Hugging Face para cargar los datos de audio\n",
    "# Configuramos una frecuencia de muestreo de 16 kHz\n",
    "data = data.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# Verificar la estructura del conjunto de datos\n",
    "print(data[\"train\"])\n",
    "print(data[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee83221",
   "metadata": {},
   "source": [
    "### Verificación de la Estructura del DatasetDict y Carga de Ejemplos de Audio\n",
    "\n",
    "En esta celda, se realiza una verificación de la estructura general del **DatasetDict** y se accede a algunos ejemplos de los datos de audio para asegurarnos de que los datos han sido cargados correctamente. A continuación, se describen los pasos realizados:\n",
    "\n",
    "1. **Verificación de la estructura del DatasetDict:**\n",
    "   - Se imprime el **DatasetDict** completo para obtener una visión general de los datos organizados en los conjuntos de **entrenamiento** y **prueba**.\n",
    "\n",
    "2. **Acceso y procesamiento de ejemplos de audio:**\n",
    "   - Se accede a los primeros 3 ejemplos del conjunto de prueba (`data[\"test\"]`) para verificar que los datos de audio y las transcripciones estén correctamente cargados.\n",
    "   \n",
    "   - Para cada ejemplo:\n",
    "     - Se imprime una muestra de los primeros 10 valores del **audio** cargado, que es un array de valores de la onda sonora.\n",
    "     - Se muestra la **frecuencia de muestreo** del audio (debe ser de 16 kHz, ya que se especificó anteriormente).\n",
    "     - Se imprime la **transcripción** correspondiente al audio.\n",
    "     - Finalmente, se imprime el **tamaño** del array de audio, lo que proporciona una idea del tamaño del archivo de audio y la duración de la grabación.\n",
    "\n",
    "3. **Objetivo de esta verificación:**\n",
    "   - Asegurarse de que los archivos de audio se han cargado correctamente en el formato esperado y que las transcripciones son las correctas.\n",
    "\n",
    "Este paso es importante para validar que la conversión a `DatasetDict` y la carga de archivos de audio se realizó correctamente y que los datos están listos para el entrenamiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29dc5553-1f9e-4a54-8051-58d1c652a32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'transcription'],\n",
      "        num_rows: 531\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'transcription'],\n",
      "        num_rows: 131\n",
      "    })\n",
      "})\n",
      "Audio 1 info: [ 0.0017749   0.00098602 -0.00814052  0.00543846  0.00554781 -0.01860339\n",
      " -0.02447864 -0.01820134 -0.01587864 -0.00732416]... (muestra de los primeros 10 valores)\n",
      "Sampling rate: 16000\n",
      "Transcription: Yeah, we're all finished with drills.\n",
      "\n",
      "El tamaño del array es: (20160,)\n",
      "Audio 2 info: [-0.01337446 -0.0035201   0.0081726  -0.00229641 -0.01017429 -0.00603535\n",
      "  0.00391879  0.00044582 -0.00639861 -0.00540883]... (muestra de los primeros 10 valores)\n",
      "Sampling rate: 16000\n",
      "Transcription: Sure, 5-6.\n",
      "\n",
      "El tamaño del array es: (7040,)\n",
      "Audio 3 info: [-0.16411234 -0.17265981 -0.07533188 -0.024495    0.01370845 -0.00356321\n",
      "  0.0020101  -0.00602992 -0.00916924 -0.00242487]... (muestra de los primeros 10 valores)\n",
      "Sampling rate: 16000\n",
      "Transcription: I've got to give that my hand.\n",
      "\n",
      "El tamaño del array es: (17296,)\n"
     ]
    }
   ],
   "source": [
    "# Verificar la estructura general del DatasetDict\n",
    "print(data)\n",
    "\n",
    "# Acceder y procesar algunos ejemplos de los datos de audio\n",
    "# Esto cargará automáticamente el audio desde las rutas especificadas y lo procesará\n",
    "for i in range(3):  # Mostrar y verificar los primeros 3 ejemplos\n",
    "    # Acceder al audio y su transcripción\n",
    "    audio_data = data[\"test\"][i][\"audio\"]   # Cargar el audio desde el conjunto de prueba\n",
    "    transcription = data[\"test\"][i][\"transcription\"]  # Obtener la transcripción correspondiente\n",
    "    \n",
    "    # Mostrar los primeros 10 valores del audio como una muestra\n",
    "    print(f\"Audio {i+1} info: {audio_data['array'][:10]}... (muestra de los primeros 10 valores)\")\n",
    "    \n",
    "    # Imprimir la frecuencia de muestreo del audio\n",
    "    print(f\"Sampling rate: {audio_data['sampling_rate']}\")\n",
    "    \n",
    "    # Imprimir la transcripción correspondiente al audio\n",
    "    print(f\"Transcription: {transcription}\\n\")\n",
    "    \n",
    "    # Obtener y mostrar el tamaño del array del audio (dimensiones del archivo de audio cargado)\n",
    "    array_size = audio_data['array'].shape\n",
    "    print(\"El tamaño del array es:\", array_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea3668",
   "metadata": {},
   "source": [
    "### Preprocesamiento del Conjunto de Datos para el Modelo Whisper\n",
    "\n",
    "En esta celda, se lleva a cabo el preprocesamiento necesario para convertir los datos en un formato adecuado para ser alimentados al modelo **Whisper**. Este proceso incluye la conversión de datos de audio en características que el modelo puede interpretar y la codificación de las transcripciones como secuencias de ids de tokens.\n",
    "\n",
    "#### Pasos realizados:\n",
    "\n",
    "1. **Carga de componentes de Whisper:**\n",
    "   - **WhisperFeatureExtractor:** Extrae características log-Mel del audio, que son adecuadas para la entrada del modelo.\n",
    "   - **WhisperTokenizer:** Convierte las transcripciones de texto en ids de tokens que el modelo puede entender.\n",
    "   - **WhisperProcessor:** Combina el extractor de características y el tokenizador en un solo proceso.\n",
    "\n",
    "2. **Definición de la función `prepare_dataset`:**\n",
    "   - Esta función recibe un **batch** de datos y realiza dos transformaciones principales:\n",
    "     - **Audio:** Convierte los archivos de audio en características log-Mel con el extractor de características de Whisper. También asegura que el audio esté a una frecuencia de muestreo de 16 kHz.\n",
    "     - **Transcripción:** Convierte la transcripción de texto en una secuencia de **ids de tokens** utilizando el tokenizador de Whisper.\n",
    "   \n",
    "3. **Aplicación del preprocesamiento:**\n",
    "   - Se aplica la función `prepare_dataset` a todo el conjunto de datos utilizando el método `map()` de **Hugging Face Datasets**, lo que permite preprocesar los datos de manera eficiente.\n",
    "   - Se eliminan las columnas originales de audio y transcripción después del preprocesamiento, dejando solo las características (`input_features`) y las etiquetas (`labels`).\n",
    "\n",
    "4. **Parámetros:**\n",
    "   - **`num_proc=1`:** Este parámetro especifica el número de procesos paralelos que se utilizarán para aplicar el preprocesamiento. Se puede aumentar para optimizar el tiempo si se tienen más recursos.\n",
    "   - **`remove_columns`:** Se elimina la columna original de audio y transcripción para dejar solo los datos necesarios para el entrenamiento.\n",
    "\n",
    "#### Resultado:\n",
    "- El conjunto de datos resultante está ahora listo para ser utilizado en el entrenamiento de un modelo de transcripción automático con Whisper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0075b438-0261-4ed8-bad2-ece7dfee94ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['audio', 'transcription']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████| 531/531 [00:02<00:00, 187.83 examples/s]\n",
      "Map: 100%|████████████████████████████| 131/131 [00:00<00:00, 177.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Importar las herramientas necesarias de Hugging Face para el procesamiento de audio y texto\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "# Cargar el extractor de características de Whisper desde el modelo preentrenado\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(modelo)\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "# Cargar el tokenizador de Whisper para preprocesar las transcripciones (texto)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(modelo, language=language, task=task)\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "# Cargar el procesador que combina el extractor de características y el tokenizador\n",
    "processor = WhisperProcessor.from_pretrained(modelo, language=language, task=task)\n",
    "\n",
    "# Función para preparar y preprocesar los datos del dataset\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"\n",
    "    Esta función procesa un lote de datos de audio y transcripciones:\n",
    "    - Convierte el audio en características log-Mel.\n",
    "    - Codifica el texto de la transcripción en ids de tokens.\n",
    "\n",
    "    Args:\n",
    "        batch (dict): Un lote de datos que contiene audio y transcripción.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario con las características del audio y las transcripciones codificadas.\n",
    "    \"\"\"\n",
    "    # Cargar y re-muestrear los datos de audio a 16 kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Extraer características log-Mel desde el audio\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # Codificar el texto de la transcripción en ids de tokens (labels)\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# Verificar las columnas del conjunto de datos de entrenamiento\n",
    "print(data.column_names[\"train\"])\n",
    "\n",
    "# Aplicar el preprocesamiento a los conjuntos de entrenamiento y prueba\n",
    "# La función 'prepare_dataset' se aplica a cada elemento del conjunto de datos\n",
    "# Se eliminan las columnas originales de audio y transcripción del dataset\n",
    "processed_dataset = data.map(prepare_dataset, remove_columns=data.column_names[\"train\"], num_proc=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c711929-a474-4c25-b82a-9aedecb30eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': './train/17329-20230629-0004-002.wav', 'array': array([-0.01159347, -0.00845765,  0.0174444 , ...,  0.00283612,\n",
      "        0.00925337, -0.00678982]), 'sampling_rate': 16000}, 'transcription': 'Can we get a split, please?'}\n",
      "{'audio': {'path': './test/17329-20230629-0004-001.wav', 'array': array([ 0.0017749 ,  0.00098602, -0.00814052, ..., -0.03541334,\n",
      "       -0.02834846, -0.00456005]), 'sampling_rate': 16000}, 'transcription': \"Yeah, we're all finished with drills.\"}\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][0])\n",
    "print(data[\"test\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79caab9",
   "metadata": {},
   "source": [
    "### Preprocesamiento del Conjunto de Datos para el Modelo Whisper\n",
    "\n",
    "En esta celda, se lleva a cabo el preprocesamiento necesario para convertir los datos en un formato adecuado para ser alimentados al modelo **Whisper**. Este proceso incluye la conversión de datos de audio en características que el modelo puede interpretar y la codificación de las transcripciones como secuencias de **ids de tokens**.\n",
    "\n",
    "#### Pasos realizados:\n",
    "\n",
    "1. **Carga de componentes de Whisper:**\n",
    "   - **WhisperFeatureExtractor:** Extrae características log-Mel del audio, que son adecuadas para la entrada del modelo.\n",
    "   - **WhisperTokenizer:** Convierte las transcripciones de texto en **ids de tokens** que el modelo puede entender.\n",
    "   - **WhisperProcessor:** Combina el extractor de características y el tokenizador en un solo proceso.\n",
    "\n",
    "2. **Definición de la función `prepare_dataset`:**\n",
    "   - Esta función recibe un **batch** de datos y realiza dos transformaciones principales:\n",
    "     - **Audio:** Convierte los archivos de audio en características log-Mel con el extractor de características de Whisper. También asegura que el audio esté a una frecuencia de muestreo de **16 kHz**.\n",
    "     - **Transcripción:** Convierte la transcripción de texto en una secuencia de **ids de tokens** utilizando el tokenizador de Whisper.\n",
    "   \n",
    "3. **Aplicación del preprocesamiento:**\n",
    "   - Se aplica la función `prepare_dataset` a todo el conjunto de datos utilizando el método `map()` de **Hugging Face Datasets**, lo que permite preprocesar los datos de manera eficiente.\n",
    "   - Se eliminan las columnas originales de **audio** y **transcripción** después del preprocesamiento, dejando solo las características (**input_features**) y las etiquetas (**labels**).\n",
    "\n",
    "4. **Parámetros:**\n",
    "   - **`num_proc=1`:** Este parámetro especifica el número de procesos paralelos que se utilizarán para aplicar el preprocesamiento. Se puede aumentar para optimizar el tiempo si se tienen más recursos.\n",
    "   - **`remove_columns`:** Se elimina la columna original de **audio** y **transcripción** para dejar solo los datos necesarios para el entrenamiento.\n",
    "\n",
    "#### Resultado:\n",
    "El conjunto de datos resultante está ahora listo para ser utilizado en el entrenamiento de un modelo de transcripción automático con **Whisper**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4acd7e76-0703-4eb7-a860-047be8bf90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# Definir un DataCollator personalizado que gestiona el padding y la preparación de los datos\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any  # El procesador utilizado para manejar audio y texto\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Esta función prepara un batch de datos de audio y transcripciones para el modelo.\n",
    "        El preprocesamiento incluye el padding de las entradas y etiquetas para que tengan tamaños uniformes.\n",
    "        \n",
    "        Args:\n",
    "            features (List[Dict]): Una lista de diccionarios con las características (audio) y etiquetas (transcripciones)\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Un diccionario con las entradas y etiquetas procesadas, listas para ser alimentadas al modelo.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Primero tratamos las entradas de audio, que son las características extraídas del audio\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        # Aplicamos padding a las características del audio y las convertimos a tensores de PyTorch\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Obtener las secuencias de etiquetas (transcripciones tokenizadas)\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # Aplicamos padding a las etiquetas (transcripciones)\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Reemplazar los tokens de padding con -100 para que el modelo los ignore durante el cálculo de la pérdida\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # Si el token de inicio de secuencia (bos_token) fue agregado en la tokenización anterior,\n",
    "        # lo eliminamos aquí, ya que se agregará nuevamente más tarde.\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # Agregamos las etiquetas procesadas al batch\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269b258",
   "metadata": {},
   "source": [
    "### Cálculo de la Métrica WER (Word Error Rate) para Evaluación\n",
    "\n",
    "En esta celda se define una función para calcular la métrica de **Word Error Rate (WER)**, que es comúnmente utilizada para evaluar la precisión de los modelos de transcripción automática. La función utiliza la biblioteca **`evaluate`** de Hugging Face para cargar y calcular esta métrica a partir de las predicciones y las transcripciones reales.\n",
    "\n",
    "#### Pasos realizados:\n",
    "\n",
    "1. **Carga de la métrica WER:**\n",
    "   - **`evaluate.load(\"wer\")`:** Se carga la métrica WER utilizando la biblioteca **`evaluate`**, la cual proporciona herramientas para calcular diversas métricas de evaluación para modelos de procesamiento de lenguaje natural.\n",
    "\n",
    "2. **Definición de la función `compute_metrics`:**\n",
    "   - Esta función recibe como argumento las predicciones realizadas por el modelo y las etiquetas reales. Luego, realiza las siguientes operaciones:\n",
    "     - **Obtenemos las predicciones y las etiquetas:** Extraemos las predicciones (`pred.predictions`) y las etiquetas verdaderas (`pred.label_ids`).\n",
    "     - **Reemplazamos los tokens de padding:** Los tokens de padding en las etiquetas se marcan con **`-100`**. Estos valores se reemplazan por el **`pad_token_id`** del tokenizador, ya que no deben ser considerados al calcular la métrica.\n",
    "     - **Decodificación de las secuencias:** Las secuencias de ids de tokens se convierten de nuevo a texto utilizando **`tokenizer.batch_decode`**, lo que convierte tanto las predicciones como las etiquetas en cadenas de texto sin tokens especiales.\n",
    "     - **Cálculo de la métrica WER:** Finalmente, se calcula el **WER** utilizando la función **`metric.compute`**, que compara las predicciones con las etiquetas y devuelve el porcentaje de error.\n",
    "\n",
    "3. **Resultado de la función:**\n",
    "   - La función devuelve un diccionario con la métrica **WER** calculada, que es la diferencia entre las palabras predichas y las reales.\n",
    "\n",
    "#### Resultado:\n",
    "La función **`compute_metrics`** permite evaluar el rendimiento del modelo de transcripción, específicamente en términos de la precisión de las palabras transcritas en comparación con las transcripciones reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1f0d119-1394-4179-971c-b218faf76de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la librería 'evaluate' para cargar la métrica WER (Word Error Rate)\n",
    "import evaluate\n",
    "\n",
    "# Cargar la métrica WER desde la librería 'evaluate'\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Definimos la función para calcular las métricas\n",
    "def compute_metrics(pred):\n",
    "    # Extraer las predicciones (IDs de los tokens) y las etiquetas verdaderas (label IDs)\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Reemplazar los valores -100 (que indican padding) por el token de padding del tokenizador\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decodificar las secuencias de predicciones y etiquetas a texto, ignorando los tokens especiales\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Calcular la métrica WER comparando las transcripciones predichas con las verdaderas\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # Devolver el resultado de la métrica WER\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5778dd28-a2fc-4361-bbdd-5234d7df3ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "# Importar la clase WhisperForConditionalGeneration de Hugging Face\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "# Cargar el modelo Whisper para generación condicional, configurando 8-bit y asignación automática de dispositivo\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    modelo,  # Especificar el nombre del modelo preentrenado\n",
    "    load_in_8bit=True,  # Cargar el modelo con precisión de 8 bits para ahorrar memoria\n",
    "    device_map=\"auto\"   # Cargar automáticamente el modelo en el dispositivo disponible (CPU/GPU)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "439c3e33-f339-49ed-9ddd-46388cdd0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la función para preparar el modelo para entrenamiento con precisión reducida\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# Preparar el modelo para entrenamiento utilizando k-bit (precisión reducida)\n",
    "model = prepare_model_for_kbit_training(model)  # El modelo se prepara para usar precisión reducida en k bits\n",
    "\n",
    "# Nota: El siguiente método es una alternativa si la función anterior no está disponible en la versión de PEFT\n",
    "# model = prepare_model_for_int8_training(model)  # Usar este método si se quiere trabajar con 8-bit de precisión\n",
    "\n",
    "# También se puede personalizar especificando la capa de embeddings de salida, aunque no se está utilizando en este ejemplo:\n",
    "# model = prepare_model_for_kbit_training(model, output_embedding_layer_name=\"proj_out\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114767ee",
   "metadata": {},
   "source": [
    "### Registro de un Hook de Forward para Requerir Gradientes en la Salida\n",
    "\n",
    "En esta celda se registra un hook en el modelo, específicamente en la capa **conv1** del codificador (**encoder**) del modelo, para asegurarse de que la salida de esta capa tendrá gradientes habilitados durante la retropropagación. Este tipo de modificaciones son útiles cuando se requiere calcular gradientes en capas específicas del modelo para tareas como la interpretación de modelos o el entrenamiento con un enfoque personalizado.\n",
    "\n",
    "#### Pasos realizados:\n",
    "\n",
    "1. **Definición del Hook `make_inputs_require_grad`:**\n",
    "   - Se define una función que será utilizada como un \"hook\" en la capa del modelo. Un hook es una función que se ejecuta durante el paso hacia adelante (**forward pass**) del modelo, y permite acceder a la entrada, la salida y otras características del modelo sin modificar el flujo general del mismo.\n",
    "   - La función **`make_inputs_require_grad`** se asegura de que la salida de la capa en la que se aplica tendrá gradientes habilitados, utilizando **`output.requires_grad_(True)`**. Esto es importante cuando se desea que una capa específica calcule gradientes, incluso si por defecto no lo hace.\n",
    "\n",
    "2. **Registro del Hook en la Capa `conv1`:**\n",
    "   - **`model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)`**: Aquí se está registrando el hook **`make_inputs_require_grad`** en la capa **`conv1`** del codificador (**encoder**) del modelo. Esto asegura que en cada pase hacia adelante por esa capa, la salida tendrá gradientes habilitados.\n",
    "   - Los hooks de forward permiten modificar el comportamiento del modelo de manera dinámica sin cambiar la arquitectura subyacente. En este caso, se está modificando cómo se manejan los gradientes en la capa **`conv1`**.\n",
    "\n",
    "#### Resultado:\n",
    "- Después de registrar este hook, cualquier salida de la capa **`conv1`** tendrá gradientes habilitados, lo que puede ser útil para técnicas como el cálculo de gradientes a través de capas específicas del modelo o para propósitos de depuración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69b741c4-31fe-4dd0-a4df-c87e3d00fee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7d8c55f2e4a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir un hook para habilitar los gradientes en la salida de la capa\n",
    "def make_inputs_require_grad(module, input, output):\n",
    "    output.requires_grad_(True)\n",
    "\n",
    "# Registrar el hook en la capa conv1 del codificador (encoder) del modelo\n",
    "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056544ee",
   "metadata": {},
   "source": [
    "### Configuración y Aplicación de LoRA en el Modelo\n",
    "\n",
    "En esta celda, se configura y aplica LoRA (Low-Rank Adaptation) en el modelo preexistente. LoRA es una técnica que permite realizar ajustes eficientes en modelos preentrenados mediante la adición de matrices de bajo rango, lo cual mejora el rendimiento en tareas específicas sin tener que entrenar todo el modelo desde cero.\n",
    "\n",
    "#### Pasos realizados:\n",
    "\n",
    "1. **Configuración de LoRA (`LoraConfig`):**\n",
    "   - Se crea un objeto de configuración para LoRA mediante la clase **`LoraConfig`**.\n",
    "     - **`r=32`**: Este parámetro especifica el rango de las matrices de adaptación de bajo rango que se agregarán a las capas de atención del modelo.\n",
    "     - **`lora_alpha=64`**: Un factor de escala para las actualizaciones de LoRA, que controla la magnitud de la adaptación.\n",
    "     - **`target_modules=[\"q_proj\", \"v_proj\"]`**: Las capas del modelo objetivo que se adaptarán. En este caso, se están seleccionando las capas de proyección de consultas (**`q_proj`**) y valores (**`v_proj`**) de la atención.\n",
    "     - **`lora_dropout=0.05`**: Se aplica un pequeño valor de **dropout** (descarte aleatorio de unidades) para regularizar el entrenamiento.\n",
    "     - **`bias=\"none\"`**: Se especifica que no se debe incluir un sesgo (bias) en las capas adaptativas.\n",
    "\n",
    "2. **Aplicación de LoRA al Modelo:**\n",
    "   - **`get_peft_model(model, config)`**: La función **`get_peft_model`** aplica la configuración de LoRA al modelo preentrenado, integrando las adaptaciones de bajo rango según la configuración proporcionada.\n",
    "\n",
    "3. **Impresión de Parámetros Entrenables:**\n",
    "   - **`model.print_trainable_parameters()`**: Esta línea imprime los parámetros del modelo que son entrenables, lo que permite verificar qué partes del modelo se están ajustando durante el entrenamiento (en este caso, las capas de LoRA).\n",
    "\n",
    "#### Resultado:\n",
    "- El modelo ahora tiene configuraciones específicas de LoRA que permiten una adaptación eficiente en las capas seleccionadas sin modificar todo el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db62ec8b-04f7-48a9-b321-8b9ede58633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,728,640 || all params: 1,559,033,600 || trainable%: 1.0089\n"
     ]
    }
   ],
   "source": [
    "# Importación de las librerías necesarias para LoRA\n",
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "from torch.optim import AdamW  # Importar el optimizador AdamW\n",
    "\n",
    "# Configuración de LoRA con parámetros específicos\n",
    "config = LoraConfig(\n",
    "    r=32,  # Rango de las matrices de bajo rango\n",
    "    lora_alpha=64,  # Factor de escala\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Selección de las capas de proyección\n",
    "    lora_dropout=0.1,  # Dropout para regularización\n",
    "    bias=\"none\"  # No incluir bias\n",
    ")\n",
    "\n",
    "# Aplicar LoRA al modelo utilizando la configuración definida\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# Imprimir los parámetros entrenables del modelo\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Usar AdamW con regularización L2 (weight decay)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Ahora puedes usar este optimizador en tu ciclo de entrenamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02732983",
   "metadata": {},
   "source": [
    "### Configuración de los Parámetros de Entrenamiento con `Seq2SeqTrainingArguments`\n",
    "\n",
    "En esta celda se configuran los parámetros de entrenamiento utilizando la clase **`Seq2SeqTrainingArguments`** de la librería **Transformers**. Esta clase facilita la configuración de los hiperparámetros necesarios para entrenar un modelo de secuencia a secuencia, como el modelo Whisper o cualquier otro basado en transformadores.\n",
    "\n",
    "#### Pasos realizados:\n",
    "\n",
    "1. **`output_dir=\"reach-vb/test\"`**: Define el directorio donde se guardarán los resultados del entrenamiento, incluyendo los checkpoints y el modelo entrenado. Puedes cambiar el nombre de la carpeta según sea necesario.\n",
    "\n",
    "2. **`per_device_train_batch_size=8`**: Establece el tamaño del batch (lote) de entrenamiento por dispositivo (GPU o CPU). En este caso, se utiliza un tamaño de lote de 8 para cada dispositivo.\n",
    "\n",
    "3. **`gradient_accumulation_steps=1`**: Si se usa un tamaño de batch grande, se puede acumular el gradiente durante varios pasos antes de realizar una actualización de los pesos del modelo. En este caso, se acumula el gradiente por cada paso.\n",
    "\n",
    "4. **`learning_rate=1e-3`**: Define la tasa de aprendizaje, que controla la magnitud de las actualizaciones a los pesos del modelo. Se ha establecido a **1e-3**.\n",
    "\n",
    "5. **`warmup_steps=50`**: Especifica el número de pasos de calentamiento (warmup) para la tasa de aprendizaje. Durante estos pasos, la tasa de aprendizaje aumentará progresivamente hasta alcanzar el valor definido en **`learning_rate`**.\n",
    "\n",
    "6. **`num_train_epochs=5`**: Define el número de épocas de entrenamiento. Cada época representa una iteración completa sobre el conjunto de datos de entrenamiento.\n",
    "\n",
    "7. **`evaluation_strategy=\"steps\"`**: Define la estrategia para la evaluación del modelo. En este caso, se evalúa el modelo cada ciertos pasos de entrenamiento, no cada época.\n",
    "\n",
    "8. **`fp16=True`**: Habilita el entrenamiento con **precisión de 16 bits**, lo que puede acelerar el proceso y reducir el uso de memoria en GPUs compatibles.\n",
    "\n",
    "9. **`per_device_eval_batch_size=8`**: Establece el tamaño del batch para la evaluación, que también se ha definido en 8.\n",
    "\n",
    "10. **`generation_max_length=128`**: Establece la longitud máxima de las secuencias generadas durante la inferencia (cuando el modelo realiza predicciones).\n",
    "\n",
    "11. **`logging_steps=100`**: Define cada cuántos pasos de entrenamiento se deben registrar los logs. En este caso, cada 100 pasos.\n",
    "\n",
    "12. **`max_steps=500`**: Define el número máximo de pasos de entrenamiento. Este parámetro es útil para pruebas rápidas, pero en un entrenamiento final se debe eliminar.\n",
    "\n",
    "13. **`remove_unused_columns=False`**: Si el modelo no utiliza algunas columnas del dataset (como en el caso de **PeftModel**), se debe establecer en **False** para evitar que las columnas no utilizadas se eliminen accidentalmente.\n",
    "\n",
    "14. **`label_names=[\"labels\"]`**: Especifica el nombre de las columnas que contienen las etiquetas, que en este caso son **`labels`**.\n",
    "\n",
    "#### Resultado:\n",
    "- Los parámetros establecidos en **`Seq2SeqTrainingArguments`** configuran el entorno de entrenamiento para el modelo, permitiendo su entrenamiento eficiente con el ajuste de varios hiperparámetros, como el tamaño del lote, la tasa de aprendizaje y la cantidad de pasos de evaluación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc1fb132-328d-467e-983e-309fdf355dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importación de los argumentos de entrenamiento para modelos secuenciales\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# Configuración de los parámetros de entrenamiento para el modelo\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"reach-vb/test\",  # Directorio donde se guardarán los resultados\n",
    "    per_device_train_batch_size=8,  # Tamaño del batch por dispositivo\n",
    "    gradient_accumulation_steps=1,  # Acumulación de gradientes antes de actualización\n",
    "    learning_rate=1e-3,  # Tasa de aprendizaje\n",
    "    warmup_steps=50,  # Número de pasos para el calentamiento de la tasa de aprendizaje\n",
    "    num_train_epochs=8,  # Número de épocas de entrenamiento\n",
    "    evaluation_strategy=\"steps\",  # Evaluación cada ciertos pasos\n",
    "    fp16=True,  # Entrenamiento con precisión de 16 bits\n",
    "    per_device_eval_batch_size=8,  # Tamaño del batch para la evaluación\n",
    "    generation_max_length=128,  # Longitud máxima de las secuencias generadas\n",
    "    logging_steps=100,  # Paso cada cuántos pasos registrar logs\n",
    "    max_steps=500,  # Número máximo de pasos de entrenamiento (para pruebas)\n",
    "    remove_unused_columns=False,  # Evitar la eliminación de columnas no utilizadas\n",
    "    label_names=[\"labels\"],  # Nombre de las columnas de etiquetas\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba0eb1",
   "metadata": {},
   "source": [
    "### Configuración del Entrenador con el Callback para Guardar Solo los Pesos del Adaptador\n",
    "\n",
    "En esta celda, se configura un **`Seq2SeqTrainer`** de Hugging Face para entrenar el modelo de transcripción. Además, se implementa un **callback personalizado** para guardar solo los pesos del adaptador (**PEFT**) durante el proceso de guardado, evitando guardar los pesos completos del modelo base.\n",
    "\n",
    "#### Pasos realizados:\n",
    "\n",
    "1. **Definición de la clase `SavePeftModelCallback`:**\n",
    "   - Se crea una clase personalizada que hereda de **`TrainerCallback`** para intervenir durante el proceso de guardado del modelo.\n",
    "   - El método **`on_save`** se ejecuta cada vez que se guarda un modelo durante el entrenamiento. Aquí se guarda únicamente el adaptador (**PEFT**) y se elimina el archivo de los pesos del modelo base.\n",
    "\n",
    "2. **Lógica de Guardado del Adaptador:**\n",
    "   - Se obtiene el directorio de los checkpoints utilizando **`state.global_step`** para nombrar de manera única cada checkpoint.\n",
    "   - Se guarda el modelo adaptador en una subcarpeta llamada **`adapter_model`**.\n",
    "   - Si existen pesos del modelo base (**`pytorch_model.bin`**), se eliminan, ya que no son necesarios cuando se utiliza el adaptador.\n",
    "\n",
    "3. **Configuración del `Seq2SeqTrainer`:**\n",
    "   - Se configura el **`Seq2SeqTrainer`** para el entrenamiento con los siguientes parámetros:\n",
    "     - **`training_args`**: Argumentos de entrenamiento previamente configurados.\n",
    "     - **`model`**: El modelo de transcripción que se entrenará.\n",
    "     - **`train_dataset`** y **`eval_dataset`**: Los datasets de entrenamiento y evaluación preprocesados.\n",
    "     - **`data_collator`**: El collator que maneja el padding y las entradas al modelo.\n",
    "     - **`tokenizer`**: El tokenizador utilizado para preprocesar las entradas de texto.\n",
    "     - **`callbacks`**: Se agrega el callback **`SavePeftModelCallback`** para manejar el guardado del adaptador.\n",
    "\n",
    "4. **Desactivación de la Caché del Modelo:**\n",
    "   - **`model.config.use_cache = False`**: Se desactiva el uso de la caché durante el entrenamiento para evitar posibles problemas con los pesos del modelo adaptado. **Nota importante**: Esta opción debe ser reactivada para la inferencia.\n",
    "\n",
    "#### Resultado:\n",
    "- El modelo se entrenará utilizando el **`Seq2SeqTrainer`** con el callback personalizado que asegura que solo los pesos del adaptador sean guardados, optimizando el almacenamiento y manteniendo el tamaño reducido del modelo entrenado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00f02a3b-1e3a-48d3-90ac-857bd35c8265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_features', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# Muestra las claves de la primera entrada para inspección\n",
    "print(processed_dataset[\"train\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2c51749-3d1c-4500-b90c-c54a7fc196a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22795/2658992043.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Importación de las herramientas necesarias para el entrenamiento y el callback\n",
    "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "import os\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Definir el data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=processor.feature_extractor,  # El tokenizador que estás usando\n",
    "    model=model,  # El modelo que estás entrenando\n",
    "    padding=True,  # Asegura que se maneje el padding de forma automática\n",
    ")\n",
    "\n",
    "# Callback personalizado para guardar solo los pesos del adaptador\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Crear la ruta para el checkpoint\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        # Guardar solo los pesos del adaptador\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        # Eliminar los pesos del modelo base si existen\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        \n",
    "        return control\n",
    "\n",
    "# Configuración del Seq2SeqTrainer con el callback\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,  # Argumentos de entrenamiento configurados previamente\n",
    "    model=model,  # El modelo de transcripción que se entrenará\n",
    "    train_dataset=processed_dataset[\"train\"],  # Conjunto de datos de entrenamiento\n",
    "    eval_dataset=processed_dataset[\"test\"],  # Conjunto de datos de evaluación\n",
    "    data_collator=data_collator,  # Collator que maneja el padding\n",
    "    tokenizer=processor.feature_extractor,  # Tokenizador para preprocesar entradas\n",
    "    callbacks=[SavePeftModelCallback],  # Callback personalizado para guardar el adaptador\n",
    ")\n",
    "\n",
    "# Desactivación de la caché del modelo durante el entrenamiento\n",
    "model.config.use_cache = False  # silenciar advertencias. Activar para la inferencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "724468aa-3fa7-4d4c-b8ed-963ec7c1b6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manucerrejon/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 16:14, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.265100</td>\n",
       "      <td>0.885917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.843974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.195800</td>\n",
       "      <td>0.954193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>0.957973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.975036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.4121692199707031, metrics={'train_runtime': 977.491, 'train_samples_per_second': 4.092, 'train_steps_per_second': 0.512, 'total_flos': 8.508177340416e+18, 'train_loss': 0.4121692199707031, 'epoch': 7.462686567164179})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19895d6f-ba9c-45b2-be82-d7614ea027a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|███████████| 63.0M/63.0M [00:03<00:00, 16.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Manucn10/kaggle-v1/commit/b35ee181fdce6684923401ce5639b02ee93d5467', commit_message='Upload model', commit_description='', oid='b35ee181fdce6684923401ce5639b02ee93d5467', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Manucn10/kaggle-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='Manucn10/kaggle-v1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_id = \"kaggle-v1\"\n",
    "model.push_to_hub(peft_model_id, token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507612ba",
   "metadata": {},
   "source": [
    "### Guardado del Modelo y Procesador Entrenados\n",
    "\n",
    "En esta celda, se guarda tanto el **modelo finamente ajustado** como el **procesador** utilizado para preprocesar los datos. Esto permite reutilizar los componentes entrenados en el futuro sin necesidad de volver a entrenarlos desde cero.\n",
    "\n",
    "#### Pasos realizados:\n",
    "\n",
    "1. **Guardado del modelo:**\n",
    "   - El modelo entrenado se guarda utilizando el método **`save_pretrained()`** en la carpeta **`\"./whisper-finetuned-large-v3\"`**. Este método guarda todos los pesos y la configuración del modelo, permitiendo que se recargue más tarde para realizar inferencias o continuar el entrenamiento.\n",
    "\n",
    "2. **Guardado del procesador:**\n",
    "   - El procesador, que incluye tanto el **extractor de características** como el **tokenizador**, se guarda también en la misma carpeta. Guardar el procesador asegura que el preprocesamiento de los datos se mantenga consistente con el utilizado durante el entrenamiento.\n",
    "\n",
    "#### Resultado:\n",
    "- Ambos, el modelo y el procesador, quedan guardados en la carpeta indicada y pueden ser cargados en cualquier momento con **`from_pretrained()`** para hacer predicciones o continuar con el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acebf80f-bd10-48c7-805a-bdaf623bd970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardar el modelo y el procesador entrenados\n",
    "model.save_pretrained(\"./whisper-finetuned-large-kaggle-v1\")\n",
    "processor.save_pretrained(\"./whisper-finetuned-large-kaggle-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b41c87",
   "metadata": {},
   "source": [
    "### Carga del Modelo PEFT para Whisper\n",
    "\n",
    "En esta celda, se carga el **modelo PEFT** (Parameter Efficient Fine-Tuning) de Whisper previamente ajustado. Este proceso incluye la recuperación de los parámetros entrenados, lo que permite reutilizar el modelo sin tener que volver a entrenarlo desde cero.\n",
    "\n",
    "#### Pasos realizados:\n",
    "\n",
    "1. **Definir la ruta del modelo PEFT:**\n",
    "   - Se especifica la ruta del modelo finamente ajustado (en este caso, **`\"./whisper-finetuned-large-v3\"`**) que contiene tanto el modelo base como los parámetros PEFT.\n",
    "\n",
    "2. **Cargar la configuración del modelo PEFT:**\n",
    "   - Se carga la configuración de PEFT utilizando **`PeftConfig.from_pretrained()`**, lo que proporciona información sobre el modelo base y los parámetros ajustados.\n",
    "\n",
    "3. **Cargar el modelo base de Whisper:**\n",
    "   - Se utiliza **`WhisperForConditionalGeneration.from_pretrained()`** para cargar el modelo base de Whisper, especificando que se debe cargar en **8-bit** para ahorrar memoria y habilitar el uso automático del mapeo del dispositivo (**`device_map=\"auto\"`**).\n",
    "\n",
    "4. **Cargar el modelo PEFT:**\n",
    "   - Se carga el modelo PEFT con **`PeftModel.from_pretrained()`**, que aplica los parámetros de ajuste eficientes a los pesos del modelo base.\n",
    "\n",
    "5. **Habilitar el uso de la caché:**\n",
    "   - Se establece **`model.config.use_cache = True`** para permitir que el modelo almacene en caché las salidas intermedias, acelerando las inferencias.\n",
    "\n",
    "#### Resultado:\n",
    "- El modelo PEFT de Whisper está ahora cargado y listo para ser utilizado en tareas de inferencia o entrenamiento adicional. Gracias a PEFT, solo se ajustan un pequeño conjunto de parámetros, lo que hace que el proceso sea más eficiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73777bfd-befd-4d7d-9767-e67daca0324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
    "\n",
    "peft_model_id =\"./whisper-finetuned-large-kaggle-v1\" # Use the same model ID as before.\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f1125d",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo: Cálculo del WER (Word Error Rate)\n",
    "\n",
    "Esta celda realiza la evaluación del modelo entrenado utilizando el conjunto de datos de prueba. La evaluación se basa en el cálculo del **Word Error Rate (WER)**, que mide la discrepancia entre las transcripciones generadas y las verdaderas transcripciones de referencia.\n",
    "\n",
    "#### Pasos realizados:\n",
    "\n",
    "1. **Configuración de la Evaluación:**\n",
    "   - Se utiliza un **`DataLoader`** para cargar el conjunto de datos de prueba (`processed_dataset[\"test\"]`) en lotes de tamaño 8.\n",
    "   - **`forced_decoder_ids`** se obtiene mediante el procesador para garantizar que la generación del modelo esté condicionada por el idioma y la tarea especificados.\n",
    "   - Se crea un objeto **`BasicTextNormalizer`** para normalizar las predicciones y las referencias antes de calcular el WER, eliminando variaciones innecesarias en el texto.\n",
    "\n",
    "2. **Evaluación del Modelo:**\n",
    "   - El modelo se pone en modo **`eval()`** para desactivar el entrenamiento y reducir el uso de memoria.\n",
    "   - Durante cada paso de evaluación:\n",
    "     - Se genera la transcripción utilizando el método **`generate()`** del modelo.\n",
    "     - Las predicciones generadas se decodifican y se comparan con las etiquetas (referencias) utilizando el **tokenizador** de Whisper.\n",
    "     - Las predicciones y las referencias se almacenan para su posterior evaluación.\n",
    "\n",
    "3. **Cálculo del WER:**\n",
    "   - Se calculan dos métricas de WER:\n",
    "     - **WER**: Mide el error entre las transcripciones generadas y las originales.\n",
    "     - **Normalized WER**: Calcula el WER después de normalizar las predicciones y las referencias, eliminando variaciones de texto que no afectan el significado.\n",
    "   - Se imprime el WER normalizado y no normalizado junto con las métricas de evaluación.\n",
    "\n",
    "#### Resultado:\n",
    "- Las métricas de WER se calculan y se muestran como un indicador del rendimiento del modelo en el conjunto de datos de prueba.\n",
    "\n",
    "Este proceso es útil para verificar cómo de bien el modelo realiza las transcripciones y comparar su rendimiento con otros modelos o configuraciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc542ee-c035-423d-a864-4ee234011e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "import re\n",
    "\n",
    "# Definir un normalizador personalizado\n",
    "def custom_normalizer(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar puntuación\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # Remover espacios extra\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Otros ajustes específicos del dominio\n",
    "    text = text.replace(\"etc\", \"et cetera\")  # Ejemplo\n",
    "    return text\n",
    "\n",
    "# Cargar el DataLoader para el conjunto de prueba\n",
    "eval_dataloader = DataLoader(processed_dataset[\"test\"], batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# Forzar ids de decodificador específicos para la tarea e idioma\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
    "\n",
    "# Inicializar listas para almacenar las predicciones y referencias\n",
    "predictions = []\n",
    "references = []\n",
    "normalized_predictions = []\n",
    "normalized_references = []\n",
    "\n",
    "# Establecer el modelo en modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Iterar sobre el DataLoader de evaluación\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            # Generar transcripciones a partir del modelo\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    forced_decoder_ids=forced_decoder_ids,\n",
    "                    max_new_tokens=255,\n",
    "                    num_beams=3,  # Añadido para mejorar las predicciones\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            # Obtener las etiquetas y reemplazar los tokens de padding (-100) con el id del token de padding\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "            \n",
    "            # Decodificar las predicciones y las etiquetas\n",
    "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Almacenar las predicciones y referencias\n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_labels)\n",
    "            \n",
    "            # Normalizar las predicciones y las referencias usando el normalizador personalizado\n",
    "            normalized_predictions.extend([custom_normalizer(pred) for pred in decoded_preds])\n",
    "            normalized_references.extend([custom_normalizer(label) for label in decoded_labels])\n",
    "\n",
    "        # Liberar memoria\n",
    "        del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "\n",
    "# Filtrar referencias y predicciones vacías\n",
    "filtered_predictions = [pred for pred, ref in zip(predictions, references) if ref.strip()]\n",
    "filtered_references = [ref for ref in references if ref.strip()]\n",
    "\n",
    "filtered_normalized_predictions = [\n",
    "    pred for pred, ref in zip(normalized_predictions, normalized_references) if ref.strip()\n",
    "]\n",
    "filtered_normalized_references = [ref for ref in normalized_references if ref.strip()]\n",
    "\n",
    "# Calcular el WER\n",
    "wer = 100 * metric.compute(predictions=filtered_predictions, references=filtered_references)\n",
    "\n",
    "# Calcular el WER normalizado\n",
    "normalized_wer = 100 * metric.compute(\n",
    "    predictions=filtered_normalized_predictions,\n",
    "    references=filtered_normalized_references,\n",
    ")\n",
    "\n",
    "# Almacenar las métricas de evaluación\n",
    "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(f\"{wer=} and {normalized_wer=}\")\n",
    "print(eval_metrics)\n",
    "\n",
    "# Imprimir las palabras más mal predichas\n",
    "print(\"Most mispredicted words:\")\n",
    "for word, count in error_counter.most_common(10):  # Mostrar las 10 palabras más erróneas\n",
    "    print(f\"{word}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d64cf71-4b81-43a4-92e4-91acaa61157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22795/2445811856.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/manucerrejon/anaconda3/envs/cudaenv_manucerrejon2025/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|███████████████████████████████████████████| 17/17 [00:45<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer=49.53020134228188 and normalized_wer=46.04026845637584\n",
      "{'eval/wer': 49.53020134228188, 'eval/normalized_wer': 46.04026845637584}\n",
      "Most mispredicted words:\n",
      "to: 10 times\n",
      "the: 8 times\n",
      "you: 7 times\n",
      "thank: 5 times\n",
      "be: 5 times\n",
      "you.: 5 times\n",
      "a: 5 times\n",
      "on: 5 times\n",
      "that: 4 times\n",
      "in: 4 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "import re\n",
    "\n",
    "# Definir un normalizador personalizado\n",
    "def custom_normalizer(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar puntuación\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # Remover espacios extra\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Otros ajustes específicos del dominio\n",
    "    text = text.replace(\"etc\", \"et cetera\")  # Ejemplo\n",
    "    return text\n",
    "\n",
    "# Cargar el DataLoader para el conjunto de prueba\n",
    "eval_dataloader = DataLoader(processed_dataset[\"test\"], batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# Forzar ids de decodificador específicos para la tarea e idioma\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
    "\n",
    "# Inicializar listas para almacenar las predicciones y referencias\n",
    "predictions = []\n",
    "references = []\n",
    "normalized_predictions = []\n",
    "normalized_references = []\n",
    "\n",
    "# Crear un contador para las palabras erróneas\n",
    "error_counter = Counter()\n",
    "\n",
    "# Establecer el modelo en modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Iterar sobre el DataLoader de evaluación\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            # Generar transcripciones a partir del modelo\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    forced_decoder_ids=forced_decoder_ids,\n",
    "                    max_new_tokens=255,\n",
    "                    num_beams=3,  # Usar un número intermedio de haces (prueba con 5)\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "            # Obtener las etiquetas y reemplazar los tokens de padding (-100) con el id del token de padding\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "            \n",
    "            # Decodificar las predicciones y las etiquetas\n",
    "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Almacenar las predicciones y referencias\n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_labels)\n",
    "            \n",
    "            # Normalizar las predicciones y las referencias usando el normalizador personalizado\n",
    "            normalized_predictions.extend([custom_normalizer(pred) for pred in decoded_preds])\n",
    "            normalized_references.extend([custom_normalizer(label) for label in decoded_labels])\n",
    "\n",
    "            # Analizar las predicciones erróneas\n",
    "            for pred, ref in zip(decoded_preds, decoded_labels):\n",
    "                pred_tokens = pred.split()  # Separar por palabras\n",
    "                ref_tokens = ref.split()    # Separar por palabras\n",
    "                \n",
    "                # Compara palabra por palabra, y si son diferentes, cuenta el error\n",
    "                for p, r in zip(pred_tokens, ref_tokens):\n",
    "                    if p != r:\n",
    "                        error_counter[r] += 1  # Contar cuántas veces se ha cometido este error\n",
    "\n",
    "        # Liberar memoria\n",
    "        del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "\n",
    "# Filtrar referencias y predicciones vacías\n",
    "filtered_predictions = [pred for pred, ref in zip(predictions, references) if ref.strip()]\n",
    "filtered_references = [ref for ref in references if ref.strip()]\n",
    "\n",
    "filtered_normalized_predictions = [\n",
    "    pred for pred, ref in zip(normalized_predictions, normalized_references) if ref.strip()\n",
    "]\n",
    "filtered_normalized_references = [ref for ref in normalized_references if ref.strip()]\n",
    "\n",
    "# Calcular el WER\n",
    "wer = 100 * metric.compute(predictions=filtered_predictions, references=filtered_references)\n",
    "\n",
    "# Calcular el WER normalizado\n",
    "normalized_wer = 100 * metric.compute(\n",
    "    predictions=filtered_normalized_predictions,\n",
    "    references=filtered_normalized_references,\n",
    ")\n",
    "\n",
    "# Almacenar las métricas de evaluación\n",
    "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(f\"{wer=} and {normalized_wer=}\")\n",
    "print(eval_metrics)\n",
    "\n",
    "# Imprimir las palabras más mal predichas\n",
    "print(\"Most mispredicted words:\")\n",
    "for word, count in error_counter.most_common(10):  # Mostrar las 10 palabras más erróneas\n",
    "    print(f\"{word}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcab3d-fe5a-45fa-9223-d40d6613b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83c4eb-0e35-413a-9ec6-455627048150",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb78bf61-261e-4792-835d-7baf5521fcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=32, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=32, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=32, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=32, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=32, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=32, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "# Cargar el modelo y el procesador entrenados\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"./whisper-finetuned-large-kaggle-v1\")\n",
    "processor = WhisperProcessor.from_pretrained(\"./whisper-finetuned-large-kaggle-v1\")\n",
    "model.generation_config.language = \"spanish\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "# Configurar el modelo en modo de evaluación\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d8765d-0334-4ec7-85b4-57b8de942898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "# Cargar el modelo y el procesador entrenados\n",
    "#model = WhisperForConditionalGeneration.from_pretrained(\"./whisper-finetuned\")\n",
    "#processor = WhisperProcessor.from_pretrained(\"./whisper-finetuned\")\n",
    "\n",
    "# Configurar el modelo en modo de evaluación\n",
    "model.eval()\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    # Cargar el archivo de audio\n",
    "    audio_input, _ = sf.read(audio_path)\n",
    "    \n",
    "    # Preprocesar el archivo de audio\n",
    "    input_features = processor.feature_extractor(audio_input, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "    # Configurar el idioma del modelo (español)\n",
    "    forced_decoder_ids = processor.tokenizer.get_decoder_prompt_ids(language=\"es\", task=\"transcribe\")\n",
    "    \n",
    "    # Generar la transcripción\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "\n",
    "    # Decodificar la transcripción\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "# Ejemplo de uso\n",
    "audio_path = \"./Test/26052024_AudioConversacion_9.wav\"\n",
    "transcription = transcribe_audio(audio_path)\n",
    "print(\"Transcripción:\", transcription)\n",
    "#small entrenado (en la celda de abajo sale esto):     Sí, para pues información, sobre la 1 y media, si 45 minutos después del De Decal va a salir el \n",
    "#small entrenado con esta celda sale: Aquí para vuestra información, eh, sobre la oye ya se cuante cinco minutos después del dedicado va a salir el despues del petropec, muy bien, vamos a decir que será de uno, de dos, ¿vale? Te voy a decir un poco si hay más del dedicado o como se lo ve.\n",
    "#Transcripción: Para dar cualquier información, sobre la o y media, así 45 minutos después del decal, va a salir el del cuarto alrej, o y media más o menos, no sé si será de uno o de dos, ¿eh? ¿Te imaginas los cinco que van al decal o cómo te lo ve?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33faaea9-360b-491d-9a08-52c2af48d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comprobar modelo sin entrenar\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline,  Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"./whisper-finetuned-large-Puerto\"\n",
    "#model_id = modelo\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True,use_safetensors=True\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model.to(device)\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "   \n",
    ")\n",
    "\n",
    "sample = \"./Test/26052024_AudioConversacion_9.wav\"\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n",
    "#correcta:            Para vuestra información, sobre las 1 y media, 45 minutos después del DEDECAL, va a salir el \n",
    "#del cuarto a 3, 1 y media más o menos, no sé si será de 1 o de 2, no podrían ir los mismos \n",
    "#que iban al DEDECAL o como ustedes lo vean.\n",
    "#small sin entrenar:  Si para esta información sobre la 1 y media, si 45 minutos después de el dedical va a salir el \n",
    "#4 o 3, por 1 y media más menos. No sé si será de 1 o de 2, pues podrían ir los mismos que van al dedical o como usted lo vea.\n",
    "#small entrenado:     Sí, para pues información, sobre la 1 y media, si 45 minutos después del De Decal va a salir el \n",
    "#cuarto 13, 1 y media, más menos. No sé si será de 1, de 2, me podría venir los mismos que hay van al De Decal o como usted lo veáis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38be26-aad1-4f49-bc25-98434701f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar referencias y predicciones vacías\n",
    "filtered_predictions = [pred for pred, ref in zip(predictions, references) if ref.strip()]\n",
    "filtered_references = [ref for ref in references if ref.strip()]\n",
    "\n",
    "filtered_normalized_predictions = [\n",
    "    pred for pred, ref in zip(normalized_predictions, normalized_references) if ref.strip()\n",
    "]\n",
    "filtered_normalized_references = [ref for ref in normalized_references if ref.strip()]\n",
    "\n",
    "# Mostrar la cantidad de transcripciones comparadas\n",
    "num_comparisons = len(filtered_predictions)\n",
    "print(f\"Total de transcripciones comparadas: {num_comparisons}\")\n",
    "\n",
    "# Calcular el WER\n",
    "wer = 100 * metric.compute(predictions=filtered_predictions, references=filtered_references)\n",
    "\n",
    "# Calcular el WER normalizado\n",
    "normalized_wer = 100 * metric.compute(\n",
    "    predictions=filtered_normalized_predictions,\n",
    "    references=filtered_normalized_references,\n",
    ")\n",
    "\n",
    "# Almacenar las métricas de evaluación\n",
    "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(f\"{wer=:.2f}% and {normalized_wer=:.2f}%\")\n",
    "print(eval_metrics)\n",
    "\n",
    "# Imprimir las palabras más mal predichas\n",
    "print(\"Most mispredicted words:\")\n",
    "for word, count in error_counter.most_common(10):  # Mostrar las 10 palabras más erróneas\n",
    "    print(f\"{word}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d3baa-57a4-4464-a1e6-97c122d8c7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv_manucerrejon2025",
   "language": "python",
   "name": "cudaenv_manucerrejon2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
